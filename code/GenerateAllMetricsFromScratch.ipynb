{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got papers and the fields 10.959624584000267\n",
      "Sorted the papers by year 12.856547000003047\n",
      "Got parent fields for 0 || seconds: 27.67986079199909\n",
      "Got all of the field parent nodes 27.704988124998636\n",
      "Read In Fields and Sets, converting to a dict\n",
      "Got level counts for 0 || seconds: 37.96419929200056\n",
      "Computed the level counts for papers 37.9815546250029\n",
      "completed: 0\n",
      "time spent 37.99774875000003\n",
      "Computed Talals Metric 38.396020291998866\n",
      "Computed All Metrics, saving... 38.42350679200172\n",
      "Metrics File Written to Disk. 38.455988374997105\n"
     ]
    }
   ],
   "source": [
    "########################################################\n",
    "#################### Global Vars #######################\n",
    "########################################################\n",
    "\n",
    "# this is the environment,whether you test on local\n",
    "#   machine or run on HPC dataset\n",
    "ENV = 'test'\n",
    "\n",
    "# use this value as a threshold which fields to keep\n",
    "#    based on MAG certainty, currently as long as MAG\n",
    "#    is >50% certain of some field -- we count it\n",
    "FIELD_CONFIDENCE = 0.5\n",
    "\n",
    "import time\n",
    "START_TIME = time.monotonic()\n",
    "########################################################\n",
    "###################### Imports #########################\n",
    "########################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from io import StringIO\n",
    "import itertools\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "########################################################\n",
    "####################### Paths ##########################\n",
    "########################################################\n",
    "\n",
    "if ENV == 'HPC':\n",
    "    path = \"/scratch/aal544/\"\n",
    "    mag = \"/MAG_2021/\"\n",
    "\n",
    "if ENV == 'test':\n",
    "    path = \"../data/\"\n",
    "    mag = \"mag/\"\n",
    "\n",
    "########################################################\n",
    "########### Get the Paper-Field associations ###########\n",
    "########################################################\n",
    "\n",
    "# read in the data\n",
    "PaperMetrics = pd.read_csv(path+mag+\"PaperFieldsOfStudy.txt\", sep='\\t', header = None)\n",
    "PaperMetrics.columns = ['PID', 'PaperFields', 'confidence']\n",
    "\n",
    "# only keep the fields that we are FIELD_CONFIDENCE certain in and drop confidence since we don't need it anymore\n",
    "PaperMetrics = PaperMetrics[PaperMetrics['confidence'] > FIELD_CONFIDENCE].drop('confidence', axis = 'columns')\n",
    "\n",
    "# group by PID and keep all fields in the set of fields per paper\n",
    "PaperMetrics = PaperMetrics.groupby(by = 'PID').agg(set).reset_index()\n",
    "\n",
    "sys.stdout.write('Got papers and the fields ' + str(time.monotonic()  - START_TIME)+ '\\n')\n",
    "sys.stdout.flush()\n",
    "\n",
    "########################################################\n",
    "########### Get the Paper Publication Years ############\n",
    "########################################################\n",
    "\n",
    "# get the Papers and get the publication years for each paper\n",
    "PaperPubYear = pd.read_csv(path+mag+'Papers.txt', sep = '\\t', header= None, usecols=[0,7], dtype={7:str}).dropna()\n",
    "PaperPubYear.columns = ['PID', 'PubYear']\n",
    "\n",
    "\n",
    "def cast_year_to_int(y):\n",
    "    '''\n",
    "        paper publication year is of a funky\n",
    "        format so I created this simple function\n",
    "        to purify the year and keep it as int\n",
    "    '''\n",
    "    y = str(y)\n",
    "    y = y.replace(\"'\",\"\")\n",
    "    y = y.replace(\"\\\"\",\"\")\n",
    "    try:\n",
    "        val = int(str(y)[:4])\n",
    "    except:\n",
    "        val = 9999\n",
    "    return val\n",
    "\n",
    "# cast year to int\n",
    "PaperPubYear['PubYear'] = PaperPubYear['PubYear'].apply(cast_year_to_int)\n",
    "\n",
    "# merge all papers by years\n",
    "PaperMetrics = pd.merge(PaperMetrics, PaperPubYear, on = 'PID').sort_values(by=['PubYear'])\n",
    "\n",
    "sys.stdout.write('Sorted the papers by year ' + str(time.monotonic()  - START_TIME)+ '\\n')\n",
    "sys.stdout.flush()\n",
    "\n",
    "########################################################\n",
    "######## Extend Fields with the Parent Fields  #########\n",
    "########################################################\n",
    "\n",
    "# these are kept (FieldID : ChildID)\n",
    "FieldOfStudyChildren= pd.read_csv(path + mag + 'FieldOfStudyChildren.txt', sep = '\\t', header= None)\n",
    "\n",
    "cnt = 0\n",
    "FOStree = {}\n",
    "\n",
    "for index, row in FieldOfStudyChildren.iterrows():\n",
    "    '''\n",
    "        Generate (ChildID : {Parent1, Parent2}) sets\n",
    "    '''\n",
    "    if row[1] not in FOStree:\n",
    "            FOStree[row[1]] = set()\n",
    "    FOStree[row[1]].add(row[0])\n",
    "\n",
    "\n",
    "def get_parents(low_lvl_fields):\n",
    "    '''\n",
    "        simple BFS search applied on graph\n",
    "        to propagate nodes up and get all parent\n",
    "        nodes\n",
    "    '''\n",
    "    seen = set()\n",
    "    queue = list(low_lvl_fields)\n",
    "    global cnt\n",
    "    if cnt %10**7 == 0:\n",
    "            sys.stdout.write('Got parent fields for '+ str(cnt)+ ' || seconds: '+ str(time.monotonic() - START_TIME) +'\\n')\n",
    "            sys.stdout.flush()\n",
    "    cnt+=1\n",
    "    while queue:\n",
    "            elem =queue.pop()\n",
    "            if elem not in seen:\n",
    "                    seen.add(elem)\n",
    "                    if elem in FOStree:\n",
    "                                    queue.extend(\n",
    "                                            FOStree[elem]\n",
    "                                    )\n",
    "    return seen\n",
    "\n",
    "PaperMetrics[\"PaperFields\"] = PaperMetrics[\"PaperFields\"].apply(get_parents)\n",
    "\n",
    "sys.stdout.write('Got all of the field parent nodes ' + str(time.monotonic()  - START_TIME)+ '\\n')\n",
    "sys.stdout.flush()\n",
    "########################################################\n",
    "############## Count Fields per Level  #################\n",
    "########################################################\n",
    "\n",
    "fos_levels = pd.read_csv(path + mag +'FieldsOfStudy.txt', sep = '\\t', header= None, usecols=[0,5])\n",
    "\n",
    "sys.stdout.write(\"Read In Fields and Sets, converting to a dict\\n\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "\n",
    "lev = {}\n",
    "for index, row in fos_levels.iterrows():\n",
    "        lev[row[0]] = row[5]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "def get_levels(fields):\n",
    "        ans = [0,0,0,0,0,0]\n",
    "        global cnt\n",
    "        if cnt %(35*10**6) == 0:\n",
    "                sys.stdout.write('Got level counts for '+ str(cnt)+ ' || seconds: '+ str(time.monotonic() - START_TIME) +'\\n')\n",
    "                sys.stdout.flush()\n",
    "        cnt+=1\n",
    "\n",
    "        for i in fields:\n",
    "                ans[lev[i]]+=1\n",
    "        return ans\n",
    "\n",
    "\n",
    "PaperMetrics['LevelCounts'] = PaperMetrics['PaperFields'].apply(get_levels)\n",
    "\n",
    "sys.stdout.write('Computed the level counts for papers ' + str(time.monotonic()  - START_TIME)+ '\\n')\n",
    "\n",
    "sys.stdout.flush()\n",
    "\n",
    "########################################################\n",
    "############## Calculate Talal's Metric  ###############\n",
    "########################################################\n",
    "\n",
    "# we will keep look for how many unique field pairs\n",
    "# each paper introduces in relation to all 2-combinations\n",
    "# for all other fields that were published together\n",
    "\n",
    "\n",
    "current_year = 0\n",
    "duple_occurence_curr = set()\n",
    "duple_occurence_prev = set()\n",
    "\n",
    "records_for_papers = {}\n",
    "cnt = 0\n",
    "for i in zip(*PaperMetrics[['PID', 'PaperFields','PubYear']].to_dict(\"list\").values()):\n",
    "    if current_year!=i[-1]:\n",
    "        current_year = i[-1]\n",
    "        duple_occurence_prev = duple_occurence_curr.copy()\n",
    "\n",
    "    tp = i[1]\n",
    "    introduced_tuple = 0\n",
    "\n",
    "    for tup in itertools.combinations(tp, 2):\n",
    "        tup = tuple(sorted(tup))\n",
    "        if tup not in duple_occurence_curr:\n",
    "            duple_occurence_curr.add(tup)\n",
    "        if (not duple_occurence_prev) or (tup not in duple_occurence_prev):\n",
    "            introduced_tuple+=1\n",
    "\n",
    "    if len(tp) == 1: introduced_tuple=-1\n",
    "    records_for_papers[i[0]] = (introduced_tuple, len(tp))\n",
    "    if cnt%(10**7)==0:\n",
    "        sys.stdout.write('completed: ' + str(cnt) + '\\n')\n",
    "        sys.stdout.write('time spent ' + str(time.monotonic()  - START_TIME)+ '\\n')\n",
    "        sys.stdout.flush()\n",
    "    cnt+=1\n",
    "\n",
    "tuple_counts_df = pd.DataFrame.from_dict(records_for_papers, orient='index')\n",
    "tuple_counts_df = tuple_counts_df.reset_index()\n",
    "tuple_counts_df.columns = ['PID', 'New_Tuples','Field_Count']\n",
    "\n",
    "PaperMetrics = pd.merge(PaperMetrics,tuple_counts_df, on = 'PID' )\n",
    "\n",
    "sys.stdout.write('Computed Talals Metric ' + str(time.monotonic()  - START_TIME)+ '\\n')\n",
    "sys.stdout.flush()\n",
    "\n",
    "########################################################\n",
    "########## Get Depth and Interdisciplinarity  ##########\n",
    "########################################################\n",
    "\n",
    "# just get the max index of non 0 value in levels\n",
    "# this is how deep the researchers went\n",
    "PaperMetrics['Depth'] = PaperMetrics['LevelCounts'].apply(lambda x:np.max(np.where(x)) )\n",
    "\n",
    "# here is one possible way to use New_Tuples and All Tuples\n",
    "# df['metric'] = df['new_tuples'] / df['cnt_fields']\n",
    "\n",
    "PaperMetrics['lvl0'] = PaperMetrics['LevelCounts'].apply(lambda x: x[0])\n",
    "PaperMetrics['lvl1'] = PaperMetrics['LevelCounts'].apply(lambda x: x[1])\n",
    "PaperMetrics['lvl2'] = PaperMetrics['LevelCounts'].apply(lambda x: x[2])\n",
    "PaperMetrics['lvl3'] = PaperMetrics['LevelCounts'].apply(lambda x: x[3])\n",
    "PaperMetrics['lvl4'] = PaperMetrics['LevelCounts'].apply(lambda x: x[4])\n",
    "PaperMetrics['lvl5'] = PaperMetrics['LevelCounts'].apply(lambda x: x[5])\n",
    "\n",
    "# since we are dealing with a vector [x0,x1,x2...x5]\n",
    "# it's a reasonable statement to say that higher x0 automatically\n",
    "# makes the paper more Interdisciplinary than high x1\n",
    "# hence let's first aggregate all values by multiplying and preserving the > relation\n",
    "# and then scale it with Standard Scaler\n",
    "SCALE_FACTOR = 57\n",
    "PaperMetrics['Interdisciplinarity'] = SCALE_FACTOR*(SCALE_FACTOR*(SCALE_FACTOR*(SCALE_FACTOR*(SCALE_FACTOR*PaperMetrics['lvl0']+PaperMetrics['lvl1'])+PaperMetrics['lvl2'])+PaperMetrics['lvl3'])+PaperMetrics['lvl4'])+PaperMetrics['lvl5']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "PaperMetrics[[\"Interdisciplinarity\"]] = scaler.fit_transform(PaperMetrics[[\"Interdisciplinarity\"]])\n",
    "\n",
    "PaperMetrics = PaperMetrics.drop(['lvl0', 'lvl1', 'lvl2', 'lvl3', 'lvl4', 'lvl5'], axis = 'columns')\n",
    "\n",
    "sys.stdout.write('Computed All Metrics, saving... ' + str(time.monotonic()  - START_TIME)+ '\\n')\n",
    "sys.stdout.flush()\n",
    "\n",
    "########################################################\n",
    "################## Save the Metrics  ###################\n",
    "########################################################\n",
    "\n",
    "PaperMetrics.to_csv(path+'/FinalWriteUp.csv', index = False)\n",
    "\n",
    "sys.stdout.write('Metrics File Written to Disk. ' + str(time.monotonic()  - START_TIME)+ '\\n')\n",
    "sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
